1:"$Sreact.fragment"
2:I[99897,["/_next/static/chunks/4eaa70fe861e9598.js","/_next/static/chunks/946bdd2e6c9b7c49.js","/_next/static/chunks/6dd6809ee42e252f.js","/_next/static/chunks/f2b800ca7ae14e96.js","/_next/static/chunks/e121db38c98deeb4.js","/_next/static/chunks/0973dd923ca2781d.js","/_next/static/chunks/b397c2fd2abb1207.js","/_next/static/chunks/11dff99839ab5ba0.js"],"MotionPreset"]
3:I[22016,["/_next/static/chunks/4eaa70fe861e9598.js","/_next/static/chunks/946bdd2e6c9b7c49.js","/_next/static/chunks/6dd6809ee42e252f.js","/_next/static/chunks/f2b800ca7ae14e96.js","/_next/static/chunks/e121db38c98deeb4.js","/_next/static/chunks/0973dd923ca2781d.js","/_next/static/chunks/b397c2fd2abb1207.js","/_next/static/chunks/11dff99839ab5ba0.js"],""]
4:I[48347,["/_next/static/chunks/4eaa70fe861e9598.js","/_next/static/chunks/946bdd2e6c9b7c49.js","/_next/static/chunks/6dd6809ee42e252f.js","/_next/static/chunks/f2b800ca7ae14e96.js","/_next/static/chunks/e121db38c98deeb4.js","/_next/static/chunks/0973dd923ca2781d.js","/_next/static/chunks/b397c2fd2abb1207.js","/_next/static/chunks/11dff99839ab5ba0.js"],"default"]
5:I[72436,["/_next/static/chunks/4eaa70fe861e9598.js","/_next/static/chunks/946bdd2e6c9b7c49.js","/_next/static/chunks/6dd6809ee42e252f.js","/_next/static/chunks/f2b800ca7ae14e96.js","/_next/static/chunks/e121db38c98deeb4.js","/_next/static/chunks/0973dd923ca2781d.js","/_next/static/chunks/b397c2fd2abb1207.js","/_next/static/chunks/11dff99839ab5ba0.js"],"Separator"]
c:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"OutletBoundary"]
d:"$Sreact.suspense"
6:T4852,<!doctype html><div class="lake-content-editor-core lake-engine lake-typography-classic" data-lake-element="root" data-selection-undefined="%7B%22path%22%3A%5B%5B1%2C0%2C0%2C0%5D%2C%5B1%2C0%2C0%2C0%5D%5D%2C%22active%22%3Atrue%7D"><h2 data-lake-id="QmMzl" id="QmMzl" style="padding: 7px 0px; margin: 0px; font-weight: 700; font-size: 24px; line-height: 32px;"><span>why？</span></h2><blockquote style="margin-top: 5px; margin-bottom: 5px; padding-left: 1em; margin-left: 0px; border-left: 3px solid rgb(238, 238, 238); opacity: 0.6;"><p data-lake-id="ec3bb7405bd7550ef75e1f43c383194c" id="ec3bb7405bd7550ef75e1f43c383194c" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>BERT 可以用来干什么？</span></p></blockquote><p data-lake-id="23fbdb616f0d31557fc0272925a08c03" id="23fbdb616f0d31557fc0272925a08c03" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="c82cd921db9953498d69a5ebfed46291" id="c82cd921db9953498d69a5ebfed46291" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>处理这种非结构化的数据以及之前学习到的情感分析</span></p><p data-lake-id="552845ea0ca3ed757de48705c950835a" id="552845ea0ca3ed757de48705c950835a" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="ede620a9eeba128e8744d65d197be635" id="ede620a9eeba128e8744d65d197be635" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>BERT 可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型，</span></p><p data-lake-id="e764cf4f661a88ce43217a7f9e73c915" id="e764cf4f661a88ce43217a7f9e73c915" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><h2 data-lake-id="SoIfy" id="SoIfy" style="padding: 7px 0px; margin: 0px; font-weight: 700; font-size: 24px; line-height: 32px;"><span>what？</span></h2><p data-lake-id="750137749eeb8445f5a750d2e948f4fc" id="750137749eeb8445f5a750d2e948f4fc" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>是一个自然语言处理模型</span></p><p data-lake-id="14e39690420b1775312001f022312241" id="14e39690420b1775312001f022312241" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>提出了“训练词向量”概念，这是独特之处</span></p><p data-lake-id="fb23f27ed98d0439bfc1a3c65991a3af" id="fb23f27ed98d0439bfc1a3c65991a3af" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="196aef9c54ed8b55a864d421c3affea0" id="196aef9c54ed8b55a864d421c3affea0" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><strong><span>BERT 利用了 Transformer 的 encoder 部分。</span></strong></p><p data-lake-id="86fd545c660e9a6889a7dfefa31e1a63" id="86fd545c660e9a6889a7dfefa31e1a63" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><strong><span>BERT 的创新点在于它将双向 Transformer 用于语言模型，</span></strong></p><p data-lake-id="befa81a632ce3c61445c15128fa41d50" id="befa81a632ce3c61445c15128fa41d50" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来。实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻，Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果。BERT 的目标是生成语言模型，所以只需要 encoder 机制。Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能。 Transformer 的 encoder 部分，输入是一个 token 序列，先对其进行 embedding 称为向量，然后输入给神经网络，输出是大小为 H 的向量序列，每个向量对应着具有相同索引的 token。当我们在训练语言模型时，有一个挑战就是要定义一个预测目标，很多模型在一个序列中预测下一个单词，“The child came home from ___”双向的方法在这样的任务中是有限制的，为了克服这个问题，BERT 使用两个策略:</span></p><h3 data-lake-id="PGVlb" id="PGVlb" style="padding: 7px 0px; margin: 0px; font-weight: 700; font-size: 20px; line-height: 28px;"><span>Masked LM (MLM)</span></h3><p data-lake-id="0ee04d459d5b3c3dc4fa2a6c770cd12a" id="0ee04d459d5b3c3dc4fa2a6c770cd12a" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>论文中介绍了一种新技术叫做 Masked LM（MLM），在这个技术出现之前是无法进行双向语言模型训练的。在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。</span></p><p data-lake-id="59ad2945651a517229be802fad20586f" id="59ad2945651a517229be802fad20586f" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="5e11775ccadeae7fe2e76420dcbfdea1" id="5e11775ccadeae7fe2e76420dcbfdea1" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>这样就需要：</span></p><ul class="lake-list" data-lake-id="8c7ed3a6330fd908d0f1f447da832f18_ul_0" lake-indent="0" style="list-style-type: disc; margin: 0px; padding-left: 23px; font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word;"><li data-lake-id="1d7c69f17e4e3d51480cc39c25489780" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>在 encoder 的输出上添加一个分类层</span></li><li data-lake-id="29e80f48b8b6677bcfc9cd1e6ab76069" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>用嵌入矩阵乘以输出向量，将其转换为词汇的维度</span></li><li data-lake-id="852de69aa7e6fc0ef176ef97941b4c14" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>用 softmax 计算词汇表中每个单词的概率</span></li></ul><p data-lake-id="e7423df12dc3f78076db3b90884ce5a6" id="e7423df12dc3f78076db3b90884ce5a6" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="6bf72040d7b344f5b61fa736b4457dfc" id="6bf72040d7b344f5b61fa736b4457dfc" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。</span></p><p data-lake-id="615fc7a3da7d7b28434e75a86a6feed3" id="615fc7a3da7d7b28434e75a86a6feed3" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="04c8c4500374f0fe342ab27acfa02378" id="04c8c4500374f0fe342ab27acfa02378" style="text-align: left; font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><strong><span>eg：my dog is hairy</span></strong></p><ul data-lake-id="09f4cf3a75546f17ed3a193a946d3c40_ul_1" lake-indent="0" style="list-style-type: disc; margin: 0px; padding-left: 23px; font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word;"><li data-lake-id="496115badd4393942914d31debfb699e"><span>有80%的概率用“[mask]”标记来替换——my dog is [MASK]</span></li><li data-lake-id="349a2c6494ddbcca7de20b70ebe7b826"><span>有10%的概率用随机采样的一个单词来替换——my dog is apple</span></li><li data-lake-id="f6a32b7016ecd937ba3851dc4e477ee8"><span>有10%的概率不做替换——my dog is hairy</span></li></ul><h3 data-lake-id="89DVm" id="89DVm" style="padding: 7px 0px; margin: 0px; font-weight: 700; font-size: 20px; line-height: 28px;"><span>Next Sentence Prediction (NSP)</span></h3><p data-lake-id="75e65e079964114273708a308f723c12" id="75e65e079964114273708a308f723c12" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：</span></p><p data-lake-id="33027e6a488dc7c4a68d923794c2f552" id="33027e6a488dc7c4a68d923794c2f552" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><ul class="lake-list" data-lake-id="6d0a401c8bb2d48b070cce9f584db5c9_ul_2" lake-indent="0" style="list-style-type: disc; margin: 0px; padding-left: 23px; font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word;"><li data-lake-id="540700a6986c2c94c03c58d4f1d6ee73" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。</span></li><li data-lake-id="bd720e6295fc2ea40133ee1c0bf0d8c9" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。</span></li><li data-lake-id="d41c0a90ff6a24f0751e06eaf0b76f12" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>给每个 token 添加一个位置 embedding，来表示它在序列中的位置。</span></li><li data-lake-id="17c3fc6224d25dbbce673c8eb4af22c7" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：</span></li><li data-lake-id="f2f91655eda5e3968c23da52d4185e45" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>整个输入序列输入给 Transformer 模型</span></li><li data-lake-id="bb780f1f3ebc46947923155094495131" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量</span></li><li data-lake-id="3d20f16be54396b5896fd49c755d7ac7" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>用 softmax 计算 IsNextSequence 的概率</span></li></ul><p data-lake-id="b89a08c9e01239cdf586fae257b063ed" id="b89a08c9e01239cdf586fae257b063ed" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="22124aeac3683100b16b15985885efd9" id="22124aeac3683100b16b15985885efd9" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。</span></p><h2 data-lake-id="EkXye" id="EkXye" style="padding: 7px 0px; margin: 0px; font-weight: 700; font-size: 24px; line-height: 32px;"><span>how？</span></h2><p data-lake-id="79f6188d4282c40c16ed6aff02fadf39" id="79f6188d4282c40c16ed6aff02fadf39" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>BERT 可以用于各种NLP任务，只需在核心模型中添加一个层.</span></p><p data-lake-id="26b59c6f69d7ee7d2df66f86de875eff" id="26b59c6f69d7ee7d2df66f86de875eff" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><span>例如：</span></p><ul class="lake-list" data-lake-id="6accca962a7aedb3ce49c489ef714cfd_ul_3" lake-indent="0" style="list-style-type: disc; margin: 0px; padding-left: 23px; font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word;"><li data-lake-id="0e6fe1a94b8968aff7e52ed6f92a7f3a" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层</span></li><li data-lake-id="d08b109a55e20e3bcb68ef55f591e4e3" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。</span></li><li data-lake-id="385d7efda04fdaacc41cbd46c063b79c" class="lake-list-node lake-list-task" data-lake-checked="true" style="list-style: none;"><span data-card-type="inline" data-lake-card="checkbox" contenteditable="false"><span style="margin: 3px 0.5ex; vertical-align: middle; width: 16px; height: 16px;">✅<span></span></span></span><span>在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。</span></li></ul><p data-lake-id="834b34a0a59600348c97c2712968d078" id="834b34a0a59600348c97c2712968d078" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p><p data-lake-id="83250b8e71dd4b733e4b46c63f63a637" id="83250b8e71dd4b733e4b46c63f63a637" style="font-size: 14px; color: rgb(38, 38, 38); line-height: 1.74; letter-spacing: 0.05em; outline-style: none; overflow-wrap: break-word; margin: 0px;"><br></p></div>0:{"buildId":"IZGNs8L3bI3loAyUuinBD","rsc":["$","$1","c",{"children":[["$","section",null,{"className":"relative","children":["$","$L2",null,{"fade":true,"blur":true,"transition":{"duration":0.5},"delay":0.1,"className":"relative overflow-hidden border-y xl:flex","children":[["$","div",null,{"className":"m-6 w-full flex-1 bg-[radial-gradient(circle_at_center,color-mix(in_oklab,var(--primary)_15%,transparent)_2px,transparent_2px)] bg-size-[18px_18px] max-xl:hidden"}],["$","div",null,{"className":"mx-auto w-full max-w-6xl flex-none space-y-8 px-4 py-8 min-[1158px]:border-x sm:px-6 sm:py-12 lg:px-8","children":[["$","$L2",null,{"fade":true,"blur":true,"slide":{"direction":"down","offset":30},"transition":{"duration":0.4},"children":["$","$L3",null,{"href":"/blog","className":"text-muted-foreground hover:text-foreground inline-flex items-center gap-2 text-sm transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left h-4 w-4","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],"返回","技术博客"]}]}],["$","article",null,{"className":"mx-auto max-w-4xl","children":[["$","header",null,{"className":"mb-8 space-y-6","children":["",["$","$L2",null,{"delay":0.3,"transition":{"duration":0.5},"children":["$","h1",null,{"className":"text-2xl font-bold tracking-tight sm:text-3xl md:text-4xl","children":["$","$L4",null,{"text":"对谷歌BERT模型的思考","delay":30,"animateBy":"words","direction":"bottom"}]}]}],["$","$L2",null,{"fade":true,"blur":true,"slide":{"direction":"down","offset":30},"delay":0.4,"transition":{"duration":0.5},"children":["$","p",null,{"className":"text-muted-foreground text-lg","children":"why？BERT 可以用来干什么？处理这种非结构化的数据以及之前学习到的情感分析BERT 可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型，what？是一个自然语言处理模型提出了“训练词向量”概念，这是独特之处BERT 利用了 Tran..."}]}],["$","$L2",null,{"fade":true,"blur":true,"slide":{"direction":"down","offset":30},"delay":0.5,"transition":{"duration":0.5},"children":["$","div",null,{"className":"text-muted-foreground flex flex-wrap items-center gap-4 text-sm","children":[["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar h-4 w-4","aria-hidden":"true","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],"$undefined"]}],["$","span",null,{"children":"2020年8月30日"}]]}],["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock h-4 w-4","aria-hidden":"true","children":[["$","path","mmk7yg",{"d":"M12 6v6l4 2"}],["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],"$undefined"]}],["$","span",null,{"children":"1.4千字"}]]}],["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-eye h-4 w-4","aria-hidden":"true","children":[["$","path","1nclc0",{"d":"M2.062 12.348a1 1 0 0 1 0-.696 10.75 10.75 0 0 1 19.876 0 1 1 0 0 1 0 .696 10.75 10.75 0 0 1-19.876 0"}],["$","circle","1v7zrd",{"cx":"12","cy":"12","r":"3"}],"$undefined"]}],["$","span",null,{"children":[19," 阅读"]}]]}],false,false]}]}],false]}],["$","$L2",null,{"delay":0.6,"fade":true,"blur":true,"transition":{"duration":0.6},"className":"-mx-4 sm:-mx-6 lg:-mx-8","children":["$","$L5",null,{}]}],["$","$L2",null,{"fade":true,"blur":true,"slide":{"direction":"up","offset":50},"delay":0.7,"transition":{"duration":0.6},"children":["$","div",null,{"className":"prose prose-neutral dark:prose-invert mt-8 max-w-none","dangerouslySetInnerHTML":{"__html":"$6"}}]}],"$L7","$L8"]}]]}],"$L9"]}]}],["$La"],"$Lb"]}],"loading":null,"isPartial":false}
7:["$","$L2",null,{"delay":0.8,"fade":true,"blur":true,"transition":{"duration":0.6},"className":"-mx-4 mt-12 sm:-mx-6 lg:-mx-8","children":["$","$L5",null,{}]}]
8:["$","$L2",null,{"fade":true,"blur":true,"slide":{"direction":"up","offset":30},"delay":0.9,"transition":{"duration":0.5},"children":["$","nav",null,{"className":"mt-8 grid gap-4 md:grid-cols-2","children":[["$","$L3",null,{"href":"/blog/fgh5bc","className":"hover:bg-muted group flex flex-col rounded-lg border p-4 transition-colors","children":[["$","span",null,{"className":"text-muted-foreground mb-2 text-sm","children":"上一篇"}],["$","span",null,{"className":"group-hover:text-primary line-clamp-2 font-medium transition-colors","children":"IDEA配置Tomcat"}]]}],["$","$L3",null,{"href":"/blog/yo1xhz","className":"hover:bg-muted group flex flex-col rounded-lg border p-4 text-right transition-colors md:text-right","children":[["$","span",null,{"className":"text-muted-foreground mb-2 text-sm","children":"下一篇"}],["$","span",null,{"className":"group-hover:text-primary line-clamp-2 font-medium transition-colors","children":"Node环境+Hexo环境的搭建"}]]}]]}]}]
9:["$","div",null,{"className":"m-6 w-full flex-1 bg-[radial-gradient(circle_at_center,color-mix(in_oklab,var(--primary)_15%,transparent)_2px,transparent_2px)] bg-size-[18px_18px] max-xl:hidden"}]
a:["$","script","script-0",{"src":"/_next/static/chunks/11dff99839ab5ba0.js","async":true}]
b:["$","$Lc",null,{"children":["$","$d",null,{"name":"Next.MetadataOutlet","children":"$@e"}]}]
e:null
